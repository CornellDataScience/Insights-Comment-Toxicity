{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Toxicity Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bert_sklearn import BertClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in all datasets\n",
    "Here, we read in our datasets. We're only going to use the training set given by Kaggle, and we'll eventually end up splitting up the data with a 5% testing and 95% training split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "In this step, we clean up the data. We use Python's regex library to do that (it takes in a regular expression and transforms it to the programmer's specifications.) For now, we only really want letters in our dataset - anything else is going to be replaced by a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z]')\n",
    "fixed = []\n",
    "size = len(train_set.index)-1\n",
    "size_that_my_computer_can_handle = 5000\n",
    "starting_point = 0\n",
    "for i in range(starting_point, starting_point+size_that_my_computer_can_handle):\n",
    "    temp = regex.sub(' ', train_set['comment_text'][i])\n",
    "    temp = temp.lower()\n",
    "    fixed.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TfidfVectorizer().fit_transform(fixed).toarray()\n",
    "y_tox = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,2].values\n",
    "y_sev_tox = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,3].values\n",
    "y_obs = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,4].values\n",
    "y_threat = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,5].values\n",
    "y_insult = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,6].values\n",
    "y_hate = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,7].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "Here, we assign, then run (in the next step) the Gaussian Naive Bayes model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {'y_tox' : y_tox, \n",
    "         'y_sev_tox' : y_sev_tox, \n",
    "         'y_obs' : y_obs, \n",
    "         'y_threat' : y_threat, \n",
    "         'y_insult' : y_insult, \n",
    "         'y_hate' : y_hate}\n",
    "\n",
    "gnb_models = {'y_tox' : GaussianNB(), \n",
    "          'y_sev_tox' : GaussianNB(),\n",
    "          'y_obs' : GaussianNB(),\n",
    "          'y_threat' : GaussianNB(),\n",
    "          'y_insult' : GaussianNB(),\n",
    "          'y_hate' : GaussianNB()}\n",
    "\n",
    "preds = {}\n",
    "\n",
    "test_names = ['y_tox', 'y_sev_tox', 'y_obs', 'y_threat', 'y_insult', 'y_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "training time: 1.6362462043762207 s\n",
      "test time: 0.6083252429962158 s\n",
      "[[1034  113]\n",
      " [  59   44]]\n",
      "0.8624\n",
      "y_sev_tox\n",
      "training time: 1.6341707706451416 s\n",
      "test time: 0.6078751087188721 s\n",
      "[[1220    9]\n",
      " [  20    1]]\n",
      "0.9768\n",
      "y_obs\n",
      "training time: 1.5643830299377441 s\n",
      "test time: 0.6018581390380859 s\n",
      "[[1113   69]\n",
      " [  49   19]]\n",
      "0.9056\n",
      "y_threat\n",
      "training time: 1.587792158126831 s\n",
      "test time: 0.5936472415924072 s\n",
      "[[1238    3]\n",
      " [   9    0]]\n",
      "0.9904\n",
      "y_insult\n",
      "training time: 1.5909390449523926 s\n",
      "test time: 0.614112138748169 s\n",
      "[[1130   55]\n",
      " [  51   14]]\n",
      "0.9152\n",
      "y_hate\n",
      "training time: 1.633971929550171 s\n",
      "test time: 0.6051449775695801 s\n",
      "[[1221   15]\n",
      " [  14    0]]\n",
      "0.9768\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    gnb_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = gnb_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model\n",
    "Here, we assign, then run (in the next step) the Support Vector Machine model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_models = {'y_tox' : SVC(gamma = 'scale'), \n",
    "          'y_sev_tox' : SVC(gamma = 'scale'),\n",
    "          'y_obs' : SVC(gamma = 'scale'),\n",
    "          'y_threat' : SVC(gamma = 'scale'),\n",
    "          'y_insult' : SVC(gamma = 'scale'),\n",
    "          'y_hate' : SVC(gamma = 'scale')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "training time: 268.9495961666107 s\n",
      "test time: 69.3998601436615 s\n",
      "[[1111    1]\n",
      " [ 114   24]]\n",
      "0.908\n",
      "y_sev_tox\n",
      "training time: 89.44266390800476 s\n",
      "test time: 30.478352069854736 s\n",
      "[[1240    0]\n",
      " [  10    0]]\n",
      "0.992\n",
      "y_obs\n",
      "training time: 184.3041708469391 s\n",
      "test time: 60.187963247299194 s\n",
      "[[1184    0]\n",
      " [  53   13]]\n",
      "0.9576\n",
      "y_threat\n",
      "training time: 64.4728319644928 s\n",
      "test time: 21.248082876205444 s\n",
      "[[1246    0]\n",
      " [   4    0]]\n",
      "0.9968\n",
      "y_insult\n",
      "training time: 166.2928750514984 s\n",
      "test time: 55.5584282875061 s\n",
      "[[1197    2]\n",
      " [  41   10]]\n",
      "0.9656\n",
      "y_hate\n",
      "training time: 107.45139002799988 s\n",
      "test time: 35.520689725875854 s\n",
      "[[1240    0]\n",
      " [  10    0]]\n",
      "0.992\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    svm_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = svm_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors Model\n",
    "Here, we assign, then run (in the next step) the K Nearest Neighbors model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_models = {'y_tox' : KNeighborsClassifier(), \n",
    "          'y_sev_tox' : KNeighborsClassifier(),\n",
    "          'y_obs' : KNeighborsClassifier(),\n",
    "          'y_threat' : KNeighborsClassifier(),\n",
    "          'y_insult' : KNeighborsClassifier(),\n",
    "          'y_hate' : KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "training time: 3.5514402389526367 s\n",
      "test time: 130.8472011089325 s\n",
      "[[1102   11]\n",
      " [  81   56]]\n",
      "0.9264\n",
      "y_sev_tox\n",
      "training time: 3.546265125274658 s\n",
      "test time: 133.85179901123047 s\n",
      "[[1231    1]\n",
      " [  16    2]]\n",
      "0.9864\n",
      "y_obs\n",
      "training time: 3.6207759380340576 s\n",
      "test time: 132.8175048828125 s\n",
      "[[1178    6]\n",
      " [  27   39]]\n",
      "0.9736\n",
      "y_threat\n",
      "training time: 3.6073989868164062 s\n",
      "test time: 131.61208963394165 s\n",
      "[[1245    0]\n",
      " [   5    0]]\n",
      "0.996\n",
      "y_insult\n",
      "training time: 3.5680761337280273 s\n",
      "test time: 132.30676770210266 s\n",
      "[[1182    3]\n",
      " [  50   15]]\n",
      "0.9576\n",
      "y_hate\n",
      "training time: 3.5873069763183594 s\n",
      "test time: 130.76220393180847 s\n",
      "[[1242    0]\n",
      " [   8    0]]\n",
      "0.9936\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    k_nearest_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = k_nearest_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "Here, we assign, then run (in the next step) the Random Forest model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_models = {'y_tox' : RandomForestClassifier(), \n",
    "          'y_sev_tox' : RandomForestClassifier(),\n",
    "          'y_obs' : RandomForestClassifier(),\n",
    "          'y_threat' : RandomForestClassifier(),\n",
    "          'y_insult' : RandomForestClassifier(),\n",
    "          'y_hate' : RandomForestClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 5.248516798019409 s\n",
      "test time: 0.12573480606079102 s\n",
      "[[1123    1]\n",
      " [ 100   26]]\n",
      "0.9192\n",
      "y_sev_tox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 3.248354911804199 s\n",
      "test time: 0.06232595443725586 s\n",
      "[[1236    0]\n",
      " [  14    0]]\n",
      "0.9888\n",
      "y_obs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 6.632544040679932 s\n",
      "test time: 0.06646108627319336 s\n",
      "[[1189    0]\n",
      " [  46   15]]\n",
      "0.9632\n",
      "y_threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 2.1768083572387695 s\n",
      "test time: 0.06307601928710938 s\n",
      "[[1247    0]\n",
      " [   3    0]]\n",
      "0.9976\n",
      "y_insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 6.422931909561157 s\n",
      "test time: 0.06673407554626465 s\n",
      "[[1180    0]\n",
      " [  59   11]]\n",
      "0.9528\n",
      "y_hate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 3.5242860317230225 s\n",
      "test time: 0.0648648738861084 s\n",
      "[[1236    0]\n",
      " [  14    0]]\n",
      "0.9888\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    random_forest_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = random_forest_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n",
    "Here, we assign, then run (in the next step) the Decision Tree model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_models = {'y_tox' : DecisionTreeClassifier(), \n",
    "          'y_sev_tox' : DecisionTreeClassifier(),\n",
    "          'y_obs' : DecisionTreeClassifier(),\n",
    "          'y_threat' : DecisionTreeClassifier(),\n",
    "          'y_insult' : DecisionTreeClassifier(),\n",
    "          'y_hate' : DecisionTreeClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "training time: 281.47301292419434 s\n",
      "test time: 0.052870988845825195 s\n",
      "[[1095   26]\n",
      " [  61   68]]\n",
      "0.9304\n",
      "y_sev_tox\n",
      "training time: 59.574655055999756 s\n",
      "test time: 0.18314409255981445 s\n",
      "[[1231    6]\n",
      " [  11    2]]\n",
      "0.9864\n",
      "y_obs\n",
      "training time: 2393.996304988861 s\n",
      "test time: 0.05564689636230469 s\n",
      "[[1171    6]\n",
      " [  30   43]]\n",
      "0.9712\n",
      "y_threat\n",
      "training time: 16.826676845550537 s\n",
      "test time: 0.057665109634399414 s\n",
      "[[1244    0]\n",
      " [   6    0]]\n",
      "0.9952\n",
      "y_insult\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    decision_tree_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = decision_tree_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Here, we assign, then run (in the next step) the Logistic Regression model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_models = {'y_tox' : LogisticRegression(solver = 'lbfgs', max_iter = 200), \n",
    "          'y_sev_tox' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_obs' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_threat' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_insult' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_hate' : LogisticRegression(solver = 'lbfgs', max_iter = 200)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    logistic_regression_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = logistic_regression_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Classifier\n",
    "Here, we assign, then run (in the next step) the BERT Classification model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = {'y_tox' : BertClassifier(), \n",
    "          'y_sev_tox' : BertClassifier(),\n",
    "          'y_obs' : BertClassifier(),\n",
    "          'y_threat' : BertClassifier(),\n",
    "          'y_insult' : BertClassifier(),\n",
    "          'y_hate' : BertClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_names:\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    t0=time.time()\n",
    "    bert_models[i].fit(X_train, y_train)\n",
    "    print(\"training time:\", (time.time() - t0), \"s\")\n",
    "    t1=time.time()\n",
    "    preds[i] = bert_models[i].predict(X_test)\n",
    "    print(\"test time:\", (time.time() - t1), \"s\")\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
