{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Toxicity Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bert_sklearn import BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in all datasets\n",
    "Here, we read in our datasets. We're only going to use the training set given by Kaggle, and we'll eventually end up splitting up the data with a 5% testing and 95% training split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n",
      "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
      "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
      "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
      "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
      "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
      "5       00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...   \n",
      "6       0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
      "7       00031b1e95af7921  Your vandalism to the Matt Shirvington article...   \n",
      "8       00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...   \n",
      "9       00040093b2687caa  alignment on this subject and which are contra...   \n",
      "10      0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n",
      "11      00054a5e18b50dd4  bbq \\n\\nbe a man and lets discuss it-maybe ove...   \n",
      "12      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
      "13      0006f16e4e9f292e  Before you start throwing accusations and warn...   \n",
      "14      00070ef96486d6f9  Oh, and the girl above started her arguments w...   \n",
      "15      00078f8ce7eb276d  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...   \n",
      "16      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
      "17      000897889268bc93   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski   \n",
      "18      0009801bd85e5806  The Mitsurugi point made no sense - why not ar...   \n",
      "19      0009eaea3325de8c  Don't mean to bother you \\n\\nI see that you're...   \n",
      "20      000b08c464718505  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...   \n",
      "21      000bfd0867774845  \"\\nGood to know. About me, yeah, I'm studying ...   \n",
      "22      000c0dfd995809fa  \"\\n\\n Snowflakes are NOT always symmetrical! \\...   \n",
      "23      000c6a3f0cd3ba8e  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...   \n",
      "24      000cfee90f50d471  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...   \n",
      "25      000eefc67a2c930f  Radial symmetry \\n\\nSeveral now extinct lineag...   \n",
      "26      000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...   \n",
      "27      000ffab30195c5e1  Yes, because the mother of the child in the ca...   \n",
      "28      0010307a3a50a353  \"\\nOk. But it will take a bit of work but I ca...   \n",
      "29      0010833a96e1f886  \"== A barnstar for you! ==\\n\\n  The Real Life ...   \n",
      "...                  ...                                                ...   \n",
      "159541  ffa33d3122b599d6  Your absurd edits \\n\\nYour absurd edits on gre...   \n",
      "159542  ffa95244f261527f  maybe he's got better things to do than spend ...   \n",
      "159543  ffad104337fe9891  scrap that, it does meet criteria and its gone...   \n",
      "159544  ffaed63c487a2b42                                You could do worse.   \n",
      "159545  ffb268f37788a011  , 7 March 2011 (UTC)\\nAre you also User:Bmatts...   \n",
      "159546  ffb47123b2d82762  \"\\n\\nHey listen don't you ever!!!! Delete my e...   \n",
      "159547  ffb7b4c3d3ae5842                     Thank you very, very much.  ·✆   \n",
      "159548  ffb93b0a0a1e78f9                        Talkback: 15 September 2012   \n",
      "159549  ffb998f9749bd83e                         2005 (UTC)\\n 06:35, 31 Mar   \n",
      "159550  ffba5332d6b8fd14  i agree/ on another note lil wayne is a talent...   \n",
      "159551  ffbc2db4225258dd  While about half the references are from BYU-I...   \n",
      "159552  ffbcd64a71775e04  Prague Spring \\n\\nI think that Prague Spring d...   \n",
      "159553  ffbd331a3aa269b9  I see this as having been merged; undoing one ...   \n",
      "159554  ffbdbb0483ed0841  and i'm going to keep posting the stuff u dele...   \n",
      "159555  ffc2f409658571f1  \"\\n\\nHow come when you download that MP3 it's ...   \n",
      "159556  ffc671f2acdd80e1  I'll be on IRC, too, if you have a more specif...   \n",
      "159557  ffc7bbb177c3c966  It is my opinion that that happens to be off-t...   \n",
      "159558  ffca1e81aefc48ac  Please stop removing content from Wikipedia; i...   \n",
      "159559  ffca8d71d71a3fae  Image:Barack-obama-mother.jpg listed for delet...   \n",
      "159560  ffcdcb71854f6d8a  \"Editing of article without Consensus & Remova...   \n",
      "159561  ffd2e85b07b3c7e4  \"\\nNo he did not, read it again (I would have ...   \n",
      "159562  ffd72e9766c09c97  \"\\n Auto guides and the motoring press are not...   \n",
      "159563  ffe029a7c79dc7fe  \"\\nplease identify what part of BLP applies be...   \n",
      "159564  ffe897e7f7182c90  Catalan independentism is the social movement ...   \n",
      "159565  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n",
      "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
      "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
      "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
      "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
      "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
      "\n",
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0           0             0        0       0       0              0  \n",
      "1           0             0        0       0       0              0  \n",
      "2           0             0        0       0       0              0  \n",
      "3           0             0        0       0       0              0  \n",
      "4           0             0        0       0       0              0  \n",
      "5           0             0        0       0       0              0  \n",
      "6           1             1        1       0       1              0  \n",
      "7           0             0        0       0       0              0  \n",
      "8           0             0        0       0       0              0  \n",
      "9           0             0        0       0       0              0  \n",
      "10          0             0        0       0       0              0  \n",
      "11          0             0        0       0       0              0  \n",
      "12          1             0        0       0       0              0  \n",
      "13          0             0        0       0       0              0  \n",
      "14          0             0        0       0       0              0  \n",
      "15          0             0        0       0       0              0  \n",
      "16          1             0        0       0       0              0  \n",
      "17          0             0        0       0       0              0  \n",
      "18          0             0        0       0       0              0  \n",
      "19          0             0        0       0       0              0  \n",
      "20          0             0        0       0       0              0  \n",
      "21          0             0        0       0       0              0  \n",
      "22          0             0        0       0       0              0  \n",
      "23          0             0        0       0       0              0  \n",
      "24          0             0        0       0       0              0  \n",
      "25          0             0        0       0       0              0  \n",
      "26          0             0        0       0       0              0  \n",
      "27          0             0        0       0       0              0  \n",
      "28          0             0        0       0       0              0  \n",
      "29          0             0        0       0       0              0  \n",
      "...       ...           ...      ...     ...     ...            ...  \n",
      "159541      1             0        1       0       1              0  \n",
      "159542      0             0        0       0       0              0  \n",
      "159543      0             0        0       0       0              0  \n",
      "159544      0             0        0       0       0              0  \n",
      "159545      0             0        0       0       0              0  \n",
      "159546      1             0        0       0       1              0  \n",
      "159547      0             0        0       0       0              0  \n",
      "159548      0             0        0       0       0              0  \n",
      "159549      0             0        0       0       0              0  \n",
      "159550      0             0        0       0       0              0  \n",
      "159551      0             0        0       0       0              0  \n",
      "159552      0             0        0       0       0              0  \n",
      "159553      0             0        0       0       0              0  \n",
      "159554      1             0        1       0       1              0  \n",
      "159555      0             0        0       0       0              0  \n",
      "159556      0             0        0       0       0              0  \n",
      "159557      0             0        0       0       0              0  \n",
      "159558      0             0        0       0       0              0  \n",
      "159559      0             0        0       0       0              0  \n",
      "159560      0             0        0       0       0              0  \n",
      "159561      0             0        0       0       0              0  \n",
      "159562      0             0        0       0       0              0  \n",
      "159563      0             0        0       0       0              0  \n",
      "159564      0             0        0       0       0              0  \n",
      "159565      0             0        0       0       0              0  \n",
      "159566      0             0        0       0       0              0  \n",
      "159567      0             0        0       0       0              0  \n",
      "159568      0             0        0       0       0              0  \n",
      "159569      0             0        0       0       0              0  \n",
      "159570      0             0        0       0       0              0  \n",
      "\n",
      "[159571 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv(\"train.csv\")\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "In this step, we clean up the data. We use Python's regex library to do that (it takes in a regular expression and transforms it to the programmer's specifications.) For now, we only really want letters in our dataset - anything else is going to be replaced by a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z]')\n",
    "fixed = []\n",
    "size = len(train_set.index)\n",
    "size_that_my_computer_can_handle = size\n",
    "starting_point = 0\n",
    "for i in range(starting_point, starting_point+size_that_my_computer_can_handle):\n",
    "    temp = regex.sub(' ', train_set['comment_text'][i])\n",
    "    temp = temp.lower()\n",
    "    fixed.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CountVectorizer().fit_transform(fixed).toarray()\n",
    "y_tox = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,2].values\n",
    "y_sev_tox = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,3].values\n",
    "y_obs = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,4].values\n",
    "y_threat = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,5].values\n",
    "y_insult = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,6].values\n",
    "y_hate = train_set.iloc[starting_point:size_that_my_computer_can_handle+starting_point,7].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "Here, we assign, then run (in the next step) the Gaussian Naive Bayes model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {'y_tox' : y_tox, \n",
    "         'y_sev_tox' : y_sev_tox, \n",
    "         'y_obs' : y_obs, \n",
    "         'y_threat' : y_threat, \n",
    "         'y_insult' : y_insult, \n",
    "         'y_hate' : y_hate}\n",
    "\n",
    "gnb_models = {'y_tox' : GaussianNB(), \n",
    "          'y_sev_tox' : GaussianNB(),\n",
    "          'y_obs' : GaussianNB(),\n",
    "          'y_threat' : GaussianNB(),\n",
    "          'y_insult' : GaussianNB(),\n",
    "          'y_hate' : GaussianNB()}\n",
    "\n",
    "preds = {}\n",
    "\n",
    "test_names = ['y_tox', 'y_sev_tox', 'y_obs', 'y_threat', 'y_insult', 'y_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    gnb_models[i].fit(X_train, y_train)\n",
    "    preds[i] = gnb_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the vectorizer and re-running the model\n",
    "What differences can we note? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = HashingVectorizer().fit_transform(fixed).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    gnb_models[i].fit(X_train, y_train)\n",
    "    preds[i] = gnb_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the vectorizer back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CountVectorizer().fit_transform(fixed).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model\n",
    "Here, we assign, then run (in the next step) the Support Vector Machine model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_models = {'y_tox' : SVC(gamma = 'scale'), \n",
    "          'y_sev_tox' : SVC(gamma = 'scale'),\n",
    "          'y_obs' : SVC(gamma = 'scale'),\n",
    "          'y_threat' : SVC(gamma = 'scale'),\n",
    "          'y_insult' : SVC(gamma = 'scale'),\n",
    "          'y_hate' : SVC(gamma = 'scale')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "[[173   0]\n",
      " [ 26   1]]\n",
      "0.87\n",
      "y_sev_tox\n",
      "[[197   0]\n",
      " [  3   0]]\n",
      "0.985\n",
      "y_obs\n",
      "[[190   0]\n",
      " [ 10   0]]\n",
      "0.95\n",
      "y_threat\n",
      "[[199   0]\n",
      " [  1   0]]\n",
      "0.995\n",
      "y_insult\n",
      "[[184   0]\n",
      " [ 16   0]]\n",
      "0.92\n",
      "y_hate\n",
      "[[199   0]\n",
      " [  1   0]]\n",
      "0.995\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    svm_models[i].fit(X_train, y_train)\n",
    "    preds[i] = svm_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors Model\n",
    "Here, we assign, then run (in the next step) the K Nearest Neighbors model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_models = {'y_tox' : KNeighborsClassifier(), \n",
    "          'y_sev_tox' : KNeighborsClassifier(),\n",
    "          'y_obs' : KNeighborsClassifier(),\n",
    "          'y_threat' : KNeighborsClassifier(),\n",
    "          'y_insult' : KNeighborsClassifier(),\n",
    "          'y_hate' : KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "[[438  12]\n",
      " [ 41   9]]\n",
      "0.894\n",
      "y_sev_tox\n",
      "[[496   0]\n",
      " [  4   0]]\n",
      "0.992\n",
      "y_obs\n",
      "[[471   4]\n",
      " [ 21   4]]\n",
      "0.95\n",
      "y_threat\n",
      "[[499   0]\n",
      " [  1   0]]\n",
      "0.998\n",
      "y_insult\n",
      "[[467   9]\n",
      " [ 22   2]]\n",
      "0.938\n",
      "y_hate\n",
      "[[496   0]\n",
      " [  4   0]]\n",
      "0.992\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    k_nearest_models[i].fit(X_train, y_train)\n",
    "    preds[i] = k_nearest_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "Here, we assign, then run (in the next step) the Random Forest model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_models = {'y_tox' : RandomForestClassifier(), \n",
    "          'y_sev_tox' : RandomForestClassifier(),\n",
    "          'y_obs' : RandomForestClassifier(),\n",
    "          'y_threat' : RandomForestClassifier(),\n",
    "          'y_insult' : RandomForestClassifier(),\n",
    "          'y_hate' : RandomForestClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "[[85  1]\n",
      " [10  4]]\n",
      "0.89\n",
      "y_sev_tox\n",
      "[[99  1]\n",
      " [ 0  0]]\n",
      "0.99\n",
      "y_obs\n",
      "[[95  0]\n",
      " [ 4  1]]\n",
      "0.96\n",
      "y_threat\n",
      "[[99  0]\n",
      " [ 1  0]]\n",
      "0.99\n",
      "y_insult\n",
      "[[94  0]\n",
      " [ 5  1]]\n",
      "0.95\n",
      "y_hate\n",
      "[[100]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    random_forest_models[i].fit(X_train, y_train)\n",
    "    preds[i] = random_forest_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n",
    "Here, we assign, then run (in the next step) the Decision Tree model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_models = {'y_tox' : DecisionTreeClassifier(), \n",
    "          'y_sev_tox' : DecisionTreeClassifier(),\n",
    "          'y_obs' : DecisionTreeClassifier(),\n",
    "          'y_threat' : DecisionTreeClassifier(),\n",
    "          'y_insult' : DecisionTreeClassifier(),\n",
    "          'y_hate' : DecisionTreeClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "[[453   5]\n",
      " [ 28  14]]\n",
      "0.934\n",
      "y_sev_tox\n",
      "[[494   1]\n",
      " [  5   0]]\n",
      "0.988\n",
      "y_obs\n",
      "[[472   6]\n",
      " [  8  14]]\n",
      "0.972\n",
      "y_threat\n",
      "[[498   0]\n",
      " [  2   0]]\n",
      "0.996\n",
      "y_insult\n",
      "[[473   8]\n",
      " [  9  10]]\n",
      "0.966\n",
      "y_hate\n",
      "[[493   3]\n",
      " [  4   0]]\n",
      "0.986\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    decision_tree_models[i].fit(X_train, y_train)\n",
    "    preds[i] = decision_tree_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Here, we assign, then run (in the next step) the Logistic Regression model on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_models = {'y_tox' : LogisticRegression(solver = 'lbfgs', max_iter = 200), \n",
    "          'y_sev_tox' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_obs' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_threat' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_insult' : LogisticRegression(solver = 'lbfgs', max_iter = 200),\n",
    "          'y_hate' : LogisticRegression(solver = 'lbfgs', max_iter = 200)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tox\n",
      "[[447   6]\n",
      " [ 31  16]]\n",
      "0.926\n",
      "y_sev_tox\n",
      "[[497   0]\n",
      " [  2   1]]\n",
      "0.996\n",
      "y_obs\n",
      "[[475   2]\n",
      " [ 17   6]]\n",
      "0.962\n",
      "y_threat\n",
      "[[499   1]\n",
      " [  0   0]]\n",
      "0.998\n",
      "y_insult\n",
      "[[473   2]\n",
      " [ 18   7]]\n",
      "0.96\n",
      "y_hate\n",
      "[[496   1]\n",
      " [  3   0]]\n",
      "0.992\n"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    logistic_regression_models[i].fit(X_train, y_train)\n",
    "    preds[i] = logistic_regression_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn classifier...\n",
      "Building sklearn classifier...\n",
      "Building sklearn classifier...\n",
      "Building sklearn classifier...\n",
      "Building sklearn classifier...\n",
      "Building sklearn classifier...\n"
     ]
    }
   ],
   "source": [
    "bert_models = {'y_tox' : BertClassifier(), \n",
    "          'y_sev_tox' : BertClassifier(),\n",
    "          'y_obs' : BertClassifier(),\n",
    "          'y_threat' : BertClassifier(),\n",
    "          'y_insult' : BertClassifier(),\n",
    "          'y_hate' : BertClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 3974324.58B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:30<00:00, 13361205.47B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "train data size: 1350, validation data size: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [4:28:35<00:00, 73.14s/it, loss=0.35]      \n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.3504, Val loss: 0.3883, Val accy = 87.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/43 [00:36<25:21, 36.22s/it, loss=0.25]"
     ]
    }
   ],
   "source": [
    "for i in test_names:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, tests[i])\n",
    "    bert_models[i].fit(X_train, y_train)\n",
    "    preds[i] = bert_models[i].predict(X_test)\n",
    "    print(i)\n",
    "    print(confusion_matrix(y_test, preds[i]))\n",
    "    print(accuracy_score(y_test, preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
