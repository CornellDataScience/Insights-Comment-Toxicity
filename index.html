<!DOCTYPE html>
<html>

<head>
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<title>Comment Toxicity</title>
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script>
	<script>
		d3version3 = d3
		window.d3 = null
		// test it worked
		console.log('v3', d3version3.version)
	</script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.10.0/d3.min.js"></script>
	<script>
		console.log('v4', d3.version)
	</script>
	<script src='https://cdnjs.cloudflare.com/ajax/libs/d3-tip/0.7.1/d3-tip.min.js'></script>

	<script src="scripts/d3.legend.js"></script>
	<style>
		body {
			font-family: 'Open Sans', sans-serif;
			font-size: 11px;
			font-weight: 300;
			fill: #242424;
			text-align: center;
			text-shadow: 0 1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff, 0 -1px 0 #fff;
			cursor: default;
		}

		.legend {
			font-family: 'Open Sans', sans-serif;
			fill: #333333;
		}

		.tooltip {
			fill: #333333;
		}

		.axis path,
		.axis line {
			fill: none;
			stroke: #000;
			shape-rendering: crispEdges;
		}

		.x.axis path {
			display: none;
		}

		.axis .domain {
			display: none;
		}

		text {
			font: 15px arial;
		}
	</style>

</head>

<body>
	<div style="text-align: center; font-size: 48px"><strong> comment toxicity </strong></div>
	<div style="text-align: center; font-size: 24px"> a comparison of the accuracy of seven different classification models
		on a kaggle dataset containing wikipedia comments<br /><br /> </div>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> about our project </u>
		<p style="font-size: 16px"> As the Internet becomes more anonymous, the necessity of moderating toxic and problematic comments
			increases. However, companies that require this type of moderation may prioritize different things.
			Some need a more efficient algorithm, while others care about accuracy. Some may need to detect only the most severe comments, while
			others desire the most toxicity-free environment possible. To address these goals, we aim to analyze seven popular text classification
			models.</p>
	</div>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> research </u>
		<p style="font-size: 16px"> Aken, B., Risch, J., Krestel, R. & Loser, A., (2018). Challenges for toxic comment classification: An in-depth error analysis. Beuth University of Applied Sciences. <https://aclweb.org/anthology/W18-5105> <ul
				style="font-size: 16px">
				<li> Used Wikipedia dataset.</li>
				<li> Raises concerns about out-of-vocabulary words, sentence dynamics (long-range dependencies), multi-word phrases.</li>
				<li> Most models have shortcomings that lead to false negatives/positives with identifiable attributes (e.g. rhetorical questions, quotes of other comments) </li>
				</ul>
				<p style="font-size: 16px">
					Chakrabarty, N., (2019). A machine learning approach to comment toxicity classification. Jalpaiguri Government Engineering College. <https://arxiv.org/ftp/arxiv/papers/1903/1903.06765.pdf> </p> <ul style="font-size: 16px">
						<li>Used Wikipedia dataset.</li>
						<li> Applied bag-of-words using word count vectorizer; set up term-document matrix; applied tf-idf.</li>
				</p>
	</div>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> preprocessing </u>
		<p style="font-size: 16px"> First, we standardized our data by removing the punctuation and converting everything to lowercase.
			Then, to give the best accuracy possible, we decided to turn our words into numbers with three vectorizers: CountVectorizer, HashingVectorizer,
			and TfidfVectorizer. A quick run-through showed that TfidfVectorizer gave the best results, so we decided to use that.
			<br /> </p>
	</div>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> caveats </u>
		<p style="font-size: 16px"> We trained our dataset with roughly 3% of the size of the actual dataset, due to time and computer memory constraints.
			This gives somewhat artificially inflated values, due to the test sets being much smaller. (for reference, when we tried training the GNB classifier with the full dataset, the
			acccuracies were ~5% lower.) Also, with BERT, we used an sklearn wrapper that causes the training and test times to be significantly higher
			than if we'd used Google's official TF library (something that we'd like to change soon.)
			<br /> </p>
		<p style="font-size: 16px">
			<br /> </p>
	</div>
	<svg width="1080" height="500" id="svg2"></svg>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> bar chart viz </u>
		<p style="font-size: 16px"> Our bar chart visualisation compares the time taken to test and train each model in relation to accuracy of performance. All of our models are within 12% of each other in terms of accuracy, but the ones that are
			clearly the most efficient at training are Gaussian Naive Bayes, K Nearest Neighbours, Random Forest, and Logistic Regression. In terms of testing efficiency, Gaussian Naive Bayes, Random Forest, Decision Tree, and Logistic Regression perform
			the best. Overall, logistic regression seems to achieve the best balance between accuracy, training efficiency, and testing efficiency.
			<br /> </p>
		<p style="font-size: 16px">
			<br /> </p>
	</div>
	<div class="radarChart"></div>

	<script src="scripts/radarChart.js"></script>
	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> radar chart viz </u>
		<p style="font-size: 16px"> Our radar chart visualisation compares the accuracy of each model per category of toxicity. As you can see, the BERT classifier is the best at detecting the tox category; Gaussian Naive Bayes, decision trees, random
			forests are the best at detecting threats; logistic regression and K nearest neighbours are the best at detecting hate; support vector machine is the best at detecting insults; decision trees and K nearest neighbours are the best at detecting
			obscenity; and K nearest neighbours is the best at detecting severe toxicity. In general, most of the models perform uniformly and significantly well in regards to detecting hate and threats, and do far more poorly on detecting obscenity and
			toxicity.
			<br /> </p>
	</div>

	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> most toxic comments </u>
		<p style="font-size: 16px">
			1. C*CKSUCKER BEFORE YOU P*SS AROUND ON MY WORK (toxic, severe_toxic, obscene, insult) <br><br>

			2. You are gay or antisemmitian?
			Archangel WHite Tiger
			Meow! Greetingshhh!
			Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...
			1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!
			2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!
			3 - First and last warning, you f*cking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!
			Beware of the Dark Side! <br><br>

			3. I think that your a F*gget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms (toxic, obscene, threat, insult, identity-hate) <br><br>

			4. Stupid peace of sh*t stop deleting my stuff a**hole go die and fall in a hole go to hell! (toxic, severe-toxic, obscene, insult) <br><br>

			5. A pair of jew-hating weiner nazi schmucks. (toxic, obscene, insult, identity-hate) <br><br>

			6. Shut up you dumb c*nt. Your a n*gger loving, c*m cuzzling crack wh*re b*tch! (toxic, severe-toxic, obscene, insult)
			Wash your dirty penis before giving sh*t
			Wash your f*cking dirty penis before giving sh*ts (toxic, severe-toxic, obscene, insult) <br><br>

			7. You are such a d*ckwad. She did it, we all watched on TV, it's on f*cking youtube. The problem I see here is that she is giving you bl*wjobs. (toxic, severe-toxic, obscene, insult) <br><br>

			8. no
			What the f*ck did you just f*cking say about me, you little b*tch? I’ll have you know I graduated top of my class in the Navy Seals, and I’ve been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained
			in gorilla warfare and I’m the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the f*ck out with precision the likes of which has never been seen before on this Earth, mark my f*cking
			words. You think you can get away with saying that sh*t to me over the Internet? Think again, f*cker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the
			storm, maggot. The storm that wipes out the pathetic little thing you call your life. You’re f*cking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that’s just with my bare hands. Not only am I
			extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable a** off the face of the continent, you little sh*t. If only you could
			have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your f*cking tongue. But you couldn’t, you didn’t, and now you’re paying the price, you goddamn idiot. I will sh*t fury
			all over you and you will drown in it. You’re f*cking dead, kiddo. (toxic, obscene, threat, insult) <br><br>

			9. Hi mutherf*ckng wikipedia nolifer go die pl0x.
			With vänlig hälsning VOP (toxic, obscene, threat, insult) <br><br>

			10. U C*NT WHY DID U BLOCK ME!! (toxic, severe-toxic, obscene, insult)
			<br /> </p>
	</div>

	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> most common words per category </u>
		<p style="font-size: 16px"> <b> toxic </b> - f*ck (and its derivatives), bullsh*t, c*cksucker <br> <br>
			<b> severe toxic </b> - c*cksucker, n*gger, f*ck (and its derivatives) <br> <br>
			<b> obscene </b> - f*ck (and its derivatives), sh*t, b*tch <br> <br>
			<b> threat </b>- rape, shoot, motherf*cker <br> <br>
			<b> insult </b>- f*ck (and its derivatives), suck, a** <br> <br>
			<b> identity-hate </b> - b*tch, f*ggot (and variations), retarded
			<br /> </p>
	</div>

	<p style="font-size: 16px"> </p>
	<p style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> markov chain generator </u> <br />
		<iframe src="https://player.vimeo.com/video/333030994" width="640" height="480" frameborder="0" allow="autoplay; fullscreen" allowfullscreen align="center"></iframe></p>
	<p style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px">
		<u> comment detection</u> <br /><input id="myText" type="text" name="lname">
		<input type="submit" value="Submit" onclick="commentTox()"></p>
	<strong>
		<p style="font-size: 40px" id="demo"></p>
	</strong>

	<script>
		function getNots(lst) {
			var count = 0;
			for (var i = 0; i < lst.length; i++) {
				if (lst[i] = "not")
					count++;
			}
			return count;
		}

		function commentTox() {
			var str = document.getElementById("myText").value;
			var display = "Not Toxic";
			var lst = str.split(" ");
			if (lst.includes("hell") || lst.includes("ass") || lst.includes("fuck") ||
				lst.includes("shit") || lst.includes("motherfucking") || lst.includes("fucking") || lst.includes("bitch") ||
				lst.includes("motherfucker") || lst.includes("goddamn") ||
				lst.includes("bastard")) {
				if (getNots(lst) % 2 == 1) display = "Toxic";
			}

			document.getElementById("demo").innerHTML = display;
		}
	</script>

	<script>
		const data2 = [{
				'State': 'Gaussian Naive Bayes',
				'Training Time': 1.6079,
				'Testing Time': 0.6052,
				'Accuracy': 93.79
			}, {
				'State': 'Support Vector Machine',
				'Training Time': 146.8206,
				'Testing Time': 45.3989,
				'Accuracy': 96.87
			},
			{
				'State': 'K Nearest Neighbors',
				'Training Time': 3.5802,
				'Testing Time': 132.0329,
				'Accuracy': 97.23
			},
			{
				'State': 'Random Forest',
				'Training Time': 4.5756,
				'Testing Time': .07483,
				'Accuracy': 96.84
			},
			{
				'State': 'Decision Tree',
				'Training Time': 49.1713,
				'Testing Time': .07766,
				'Accuracy': 96.97
			},
			{
				'State': 'Logistic Regression',
				'Training Time': 2.7116,
				'Testing Time': .04997,
				'Accuracy': 96.4
			},
			{
				'State': 'BERT*',
				'Training Time': 158.3623,
				'Testing Time': 4.7110,
				'Accuracy': 86.4
			}
		];

		const keys = Object.keys(data2[0]).slice(1);

		const tip = d3.tip().html(d => d.value);

		const margin2 = {
				top: 40,
				right: 80,
				bottom: 20,
				left: 80
			},
			width2 = 1080,
			height2 = 500,
			innerWidth = width2 - margin2.left - margin2.right,
			innerHeight = height2 - margin2.top - margin2.bottom,
			svg2 = d3.select('body').select('#svg2').attr('width', width2).attr('height', height2)
		g = svg2.append('g').attr('transform', `translate(${margin2.left}, ${margin2.top})`);

		svg2.call(tip)

		const x0 = d3.scaleBand()
			.rangeRound([0, innerWidth])
			.paddingInner(.1);

		const x1 = d3.scaleBand()
			.padding(.05);

		const y = d3.scaleLinear()
			.rangeRound([innerHeight, 0]);

		const z = d3.scaleOrdinal()
			.range(["#EDC951", "#CC333F", "#00A0B0"]);

		x0.domain(data2.map(d => d.State));
		x1.domain(keys).rangeRound([0, x0.bandwidth()]);
		y.domain([0, d3.max(data2, d => d3.max(keys, key => d[key]))]).nice();
		console.log("ok")
		g.append('g')
			.selectAll('g')
			.data(data2)
			.enter()
			.append('g')
			.attr('transform', d => 'translate(' + x0(d.State) + ',0)')
			.selectAll('rect')
			.data(d => keys.map(key => {
				return {
					key: key,
					value: d[key]
				}
			}))
			.enter().append('rect')
			.attr('x', d => x1(d.key))
			.attr('y', d => y(d.value))
			.attr('width', x1.bandwidth())
			.attr('height', d => innerHeight - y(d.value))
			.attr('fill', d => z(d.key))
			.on('mouseover', tip.show)
			.on('mouseout', tip.hide)
		console.log("ok")
		g.append('g')
			.attr('class', 'axis-bottom')
			.attr('transform', 'translate(0,' + innerHeight + ')')
			.call(d3.axisBottom(x0));

		g.append('g')
			.attr('class', 'axis-left')
			.call(d3.axisLeft(y).ticks(null, 's'))
			.append('text')
			.attr('x', 10)
			.attr('y', y(y.ticks().pop()) + 10)
			.attr('dy', '0.32em')
			.attr('fill', '#000')
			.style('transform', 'rotate(-90deg)')
			.attr('font-weight', 'bold')
			.attr('text-anchor', 'end')
			.text('Time (s) / Accuracy (%)');

		const legend = g.append('g')
			.attr('font-family', 'sans-serif')
			.attr('font-size', 10)
			.attr('text-anchor', 'end')
			.selectAll('g')
			.data(keys.slice().reverse())
			.enter().append('g')
			.attr('transform', (d, i) => 'translate(0,' + i * 20 + ')');

		legend.append('rect')
			.attr('x', innerWidth - 19)
			.attr('width', 10)
			.attr('height', 10)
			.attr('fill', z);

		legend.append('text')
			.attr('x', innerWidth - 32)
			.attr('y', 6)
			.attr('dy', '0.32em')
			.text(d => d);
	</script>
	<script>
		var margin = {
				top: 100,
				right: 100,
				bottom: 100,
				left: 100
			},
			width = Math.min(700, window.innerWidth - 10) - margin.left - margin.right,
			height = Math.min(width, window.innerHeight - margin.top - margin.bottom - 20);
		var data = [
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.8624
				},
				{
					axis: "Support Vector Machine",
					value: 0.908
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.9264
				},
				{
					axis: "Random Forest",
					value: 0.9192
				},
				{
					axis: "Decision Tree",
					value: 0.9304
				},
				{
					axis: "Logistic Regression",
					value: 0.92
				},
				{
					axis: "BERT",
					value: 0.96
				},
			],
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.9768
				},
				{
					axis: "Support Vector Machine",
					value: 0.992
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.9864
				},
				{
					axis: "Random Forest",
					value: 0.9888
				},
				{
					axis: "Decision Tree",
					value: 0.9864
				},
				{
					axis: "Logistic Regression",
					value: 0.9848
				},
				{
					axis: "BERT",
					value: 0.84
				}
			],
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.9056
				},
				{
					axis: "Support Vector Machine",
					value: 0.9576
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.9736
				},
				{
					axis: "Random Forest",
					value: 0.9632
				},
				{
					axis: "Decision Tree",
					value: 0.9712
				},
				{
					axis: "Logistic Regression",
					value: 0.9496
				},
				{
					axis: "BERT",
					value: 0.94
				}
			],
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.9904
				},
				{
					axis: "Support Vector Machine",
					value: 0.9968
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.996
				},
				{
					axis: "Random Forest",
					value: 0.9976
				},
				{
					axis: "Decision Tree",
					value: 0.9952
				},
				{
					axis: "Logistic Regression",
					value: 0.9952
				},
				{
					axis: "BERT",
					value: 0.84
				}
			],
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.9152
				},
				{
					axis: "Support Vector Machine",
					value: 0.9656
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.9576
				},
				{
					axis: "Random Forest",
					value: 0.9528
				},
				{
					axis: "Decision Tree",
					value: 0.9504
				},
				{
					axis: "Logistic Regression",
					value: 0.9456
				},
				{
					axis: "BERT",
					value: 0.84
				}
			],
			[{
					axis: "Gaussian Naive Bayes",
					value: 0.9768
				},
				{
					axis: "Support Vector Machine",
					value: 0.992
				},
				{
					axis: "K Nearest Neighbors",
					value: 0.9936
				},
				{
					axis: "Random Forest",
					value: 0.9888
				},
				{
					axis: "Decision Tree",
					value: 0.9848
				},
				{
					axis: "Logistic Regression",
					value: 0.9888
				},
				{
					axis: "BERT",
					value: 0.84
				}
			],

		];
		var color = d3version3.scale.ordinal()
			.range(["#EDC951", "#CC333F", "#00A0B0", "#FF7F0E", "#f98b95", "#aff8ff"]);

		var radarChartOptions = {
			w: width,
			h: height,
			margin: margin,
			maxValue: 1,
			levels: 2,
			roundStrokes: true,
			color: color
		};
		//Call function to draw the Radar chart
		RadarChart(".radarChart", data, radarChartOptions);
	</script>

	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><a style="color:black" href="/bubble.html"><u> bubble graph viz </u></a>
		<p style="font-size: 16px">
			Our bubble graph visualization shows frequent profanities found in each subcategory by each classifier. This offers a more qualitative look at each classifier's performance, particularly as it relates to commonalities in flagged
			comments. The nodes are sized proportionally to word prominence relative to other words in the subcategory. Classifiers like Naive Bayes and Decision Tree seem to excel at classifying words in all categories of toxicity. A more stark divide
			is perhaps seen with classifiers like Logistic Regression and SVM, which find few common profane words in the threat and identity-based hate. This possibly indicates the classifiers are less-suited for classifying these types of toxicity.
			<br> Another observation is that words like 'fuck' and 'shit' appear in every category and don't help distinguish the groupings; rather, they help crowd out words that are likely more indicative of their categories. Taking into account
			term frequency using tf-idf may assist in finding more significant results. </p>
	</div>

	<div style="text-align: left; margin-left: 40px; margin-right: 40px; font-size: 30px"><u> future goals </u>
		<p style="font-size: 16px">
			<b> tf-idf with word frequency data: </b> As mentioned earlier, using tf-idf to find word importance would help eliminate common words that just crowd out the dataset.
			<br><br> <b> measuring precision/recall: </b> To get more information on strengths/weaknesses of each classifier.
			<br><br> <b> regression analysis: </b> A regression can provide a probability of a comment being toxic rather than a binary yes/no. This would require a different dataset.</p>
	</div>

</body>

</html>
